{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 720,
   "id": "91775eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/farshed.abdukhakimov/miniconda3/envs/main/bin/python\n",
      "farshed.abdukhakimov\n",
      "srv-01\n",
      "/home/farshed.abdukhakimov/projects/twin-polyak/experiments\n"
     ]
    }
   ],
   "source": [
    "!which python\n",
    "!whoami \n",
    "!hostname\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "id": "fbf91343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 721,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, RandomSampler\n",
    "\n",
    "import torchvision\n",
    "from torchvision.transforms import v2\n",
    "import torchvision.models as pt_models\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import scipy \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sps\n",
    "import sls\n",
    "from pt_methods import *\n",
    "import models\n",
    "\n",
    "# import utils\n",
    "\n",
    "import lightning as L\n",
    "import torchmetrics\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "id": "f1b0b398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 722,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "id": "131d1aac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'sadness': 0, 'anger': 1, 'love': 2, 'surprise': 3, 'fear': 4, 'joy': 5},\n",
       " {0: 'sadness', 1: 'anger', 2: 'love', 3: 'surprise', 4: 'fear', 5: 'joy'},\n",
       " 6)"
      ]
     },
     "execution_count": 723,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = f\"{os.getenv(\"DATASETS_DIR\")}/Emotions/train.txt\"\n",
    "\n",
    "data = pd.read_csv(path, sep=';', header=None, names=['text', 'label'], engine='python')\n",
    "        \n",
    "labels = data['label'].unique()\n",
    "\n",
    "\n",
    "label2id = {label: idx for idx, label in enumerate(labels)}\n",
    "id2label = {idx: label for idx, label in enumerate(labels)}\n",
    "num_labels = len(label2id)\n",
    "\n",
    "label2id, id2label, num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "id": "890ba64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  2293,  3698,  4083,   999, 19204,  3989,  2003, 12476,\n",
      "          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "['[CLS]', 'i', 'love', 'machine', 'learning', '!', 'token', '##ization', 'is', 'awesome', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "\n",
    "text = \"I love Machine Learning! Tokenization is awesome.\"\n",
    "encoded_text = tokenizer(text,\n",
    "                         add_special_tokens=True,\n",
    "                         padding='max_length',\n",
    "                         truncation=True,\n",
    "                         max_length=128,\n",
    "                         return_attention_mask=True,\n",
    "                         return_tensors='pt')\n",
    "print(encoded_text)\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoded_text.input_ids.squeeze(0))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "id": "fba3c6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionsDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, path: str):\n",
    "        super().__init__()\n",
    "        self.data = pd.read_csv(path, sep=';', header=None, names=['text', 'label'], engine='python')\n",
    "        \n",
    "        labels = self.data['label'].unique()\n",
    "\n",
    "        self.label2id = {label: idx for idx, label in enumerate(labels)}\n",
    "        self.id2label = {idx: label for idx, label in enumerate(labels)}\n",
    "\n",
    "        \n",
    "        self.data['label'] = [self.label2id.get(label, -1) for label in self.data['label']]\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.data.iloc[index]['text']\n",
    "        label = torch.tensor(self.data.iloc[index]['label'])\n",
    "    \n",
    "        tokens = self.tokenizer(text,\n",
    "                                add_special_tokens=True,\n",
    "                                padding='max_length',\n",
    "                                truncation=True,\n",
    "                                max_length=128,\n",
    "                                return_attention_mask=True,\n",
    "                                return_tensors='pt')\n",
    "        \n",
    "        out = {\n",
    "            'input_ids': tokens['input_ids'].squeeze(0),\n",
    "            'attention_mask': tokens['attention_mask'].squeeze(0),\n",
    "            'labels': label\n",
    "        }\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "class EmotionsDataModule(L.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, batch_size: int = 32):\n",
    "        super().__init__()\n",
    "\n",
    "        self.data_dir = f\"{os.getenv(\"DATASETS_DIR\")}/Emotions\"\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        \n",
    "        if stage  == 'fit':\n",
    "            self.train_dataset = EmotionsDataset(f\"{self.data_dir}/train.txt\")\n",
    "            self.val_dataset = EmotionsDataset(f\"{self.data_dir}/val.txt\")\n",
    "        if stage == 'test':\n",
    "            self.test_dataset = EmotionsDataset(f\"{self.data_dir}/test.txt\")\n",
    "        if stage == 'predict':\n",
    "            self.test_dataset = EmotionsDataset(f\"{self.data_dir}/test.txt\")\n",
    "            \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=2, shuffle=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=2, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=2, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "id": "93247943",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "\n",
    "class EmotionsClassifier(L.LightningModule):\n",
    "    \n",
    "    def __init__(self, num_labels, config: dict):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.config = config\n",
    "        \n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=num_labels)\n",
    "\n",
    "        for param in self.model.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for name, param in self.model.base_model.named_parameters():\n",
    "            if 'pooler' in name:\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        self.valid_acc = MulticlassAccuracy(num_classes=num_labels)\n",
    "        self.test_acc = self.valid_acc.clone()\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self.model(**batch)\n",
    "        loss = output.loss\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"step_size\", self.optimizers().param_groups[0][\"lr\"], on_step=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self.model(**batch)\n",
    "        self.valid_acc(output.logits, batch[\"labels\"])\n",
    "        self.log('valid_acc', self.valid_acc, on_epoch=True, prog_bar=True)\n",
    "        self.log('valid_loss', output.loss, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        logits = self.model(**batch).logits\n",
    "        self.test_acc(logits, batch[\"labels\"])\n",
    "        self.log('test_acc', self.test_acc, on_epoch=True) \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.config['lr'])\n",
    "        scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "            optimizer, \n",
    "            start_factor=1e-8,\n",
    "            total_iters=self.config['warmup'] * self.config['train_size']\n",
    "        )\n",
    "        return [optimizer], [scheduler]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0bad2a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "assert wandb.login() == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f08214d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch import Trainer, seed_everything\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "seed_everything(0, workers=True)\n",
    "\n",
    "wandb_logger = WandbLogger(project=\"TwinPolyak\")\n",
    "\n",
    "data_module = EmotionsDataModule(batch_size=256)\n",
    "data_module.setup('fit')\n",
    "\n",
    "config = {\n",
    "    'batch_size': data_module.batch_size,\n",
    "    'lr': 1e-5,\n",
    "    'warmup': 0.1,\n",
    "    'train_size': len(data_module.train_dataloader()),\n",
    "}\n",
    "\n",
    "model = EmotionsClassifier(num_labels=num_labels, config=config)\n",
    "\n",
    "# wandb_logger.watch(model, log='all')\n",
    "\n",
    "trainer = L.Trainer(max_epochs=100, logger=None)\n",
    "\n",
    "# try: \n",
    "trainer.validate(model=model, datamodule=data_module)\n",
    "trainer.fit(model, data_module)\n",
    "trainer.test(model=model, datamodule=data_module)\n",
    "# finally:\n",
    "#     wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b89e9f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>▃█▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▄▅▂▄▄▃▃▃▄▄▆▅▆▅▃▄▂▂▄▅▂▄▂▂▇▂▄▄▄▄▅▄▄▃▄▅▁▇</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇████</td></tr><tr><td>valid_acc</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>13</td></tr><tr><td>train_loss_epoch</td><td>1.57687</td></tr><tr><td>train_loss_step</td><td>1.4944</td></tr><tr><td>trainer/global_step</td><td>6899</td></tr><tr><td>valid_acc</td><td>0.16667</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fallen-capybara-2</strong> at: <a href='https://wandb.ai/farshed-mbzuai/TwinPolyak/runs/4pk9sn9z' target=\"_blank\">https://wandb.ai/farshed-mbzuai/TwinPolyak/runs/4pk9sn9z</a><br> View project at: <a href='https://wandb.ai/farshed-mbzuai/TwinPolyak' target=\"_blank\">https://wandb.ai/farshed-mbzuai/TwinPolyak</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250601_093415-4pk9sn9z/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11baa59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
